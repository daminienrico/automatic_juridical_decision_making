{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all necessary imports\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "import glob,re, os, sys, random\n",
    "from sklearn.model_selection import cross_val_predict,cross_validate\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from nltk.corpus import stopwords\n",
    "from random import shuffle\n",
    "random_seed = [6789, 232345, 334456, 454567, 567448, 67839, 9822276, 763454, 644543, 3454421]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(starts, ends, cases, violation):\n",
    "    facts = []\n",
    "    D = []\n",
    "    years = []\n",
    "    for case in cases:\n",
    "        contline = ''\n",
    "        year = 0\n",
    "        with open(case, 'r') as f:\n",
    "            for line in f:\n",
    "                dat = re.search('^([0-9]{1,2}\\s\\w+\\s([0-9]{4}))', line)\n",
    "                if dat != None:\n",
    "                    year = int(dat.group(2))\n",
    "                    break\n",
    "            if year>0:\n",
    "                years.append(year)\n",
    "                wr = 0\n",
    "                for line in f:\n",
    "                    if wr == 0:\n",
    "                        if re.search(starts, line) != None:\n",
    "                            wr = 1\n",
    "                    if wr == 1 and re.search(ends, line) == None:\n",
    "                        contline += line\n",
    "                        contline += '\\n'\n",
    "                    elif re.search(ends, line) != None:\n",
    "                        break\n",
    "                facts.append(contline)\n",
    "    for i in range(len(facts)):\n",
    "        D.append((facts[i], violation, years[i])) \n",
    "    return D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_parts(train_path, violation, part): #extract text from different parts\n",
    "    cases = glob.glob(train_path)\n",
    "\n",
    "    facts = []\n",
    "    D = []\n",
    "    years = []\n",
    "    \n",
    "    if part == 'relevant_law': #seprarte extraction for relevant law\n",
    "        for case in cases:\n",
    "            year = 0\n",
    "            contline = ''\n",
    "            with open(case, 'r') as f:\n",
    "                for line in f:\n",
    "                    dat = re.search('^([0-9]{1,2}\\s\\w+\\s([0-9]{4}))', line)\n",
    "                    if dat != None:\n",
    "                        year = int(dat.group(2))\n",
    "                        break\n",
    "                if year> 0:\n",
    "                    years.append(year)\n",
    "                    wr = 0\n",
    "                    for line in f:\n",
    "                        if wr == 0:\n",
    "                            if re.search('RELEVANT', line) != None:\n",
    "                                wr = 1\n",
    "                        if wr == 1 and re.search('THE LAW', line) == None and re.search('PROCEEDINGS', line) == None:\n",
    "                            contline += line\n",
    "                            contline += '\\n'\n",
    "                        elif re.search('THE LAW', line) != None or re.search('PROCEEDINGS', line) != None:\n",
    "                            break\n",
    "                    facts.append(contline)\n",
    "        for i in range(len(facts)):\n",
    "            D.append((facts[i], violation, years[i]))\n",
    "        \n",
    "    if part == 'facts':\n",
    "        starts = 'THE FACTS'\n",
    "        ends ='THE LAW'\n",
    "        D = extract_text(starts, ends, cases, violation)\n",
    "    if part == 'circumstances':\n",
    "        starts = 'CIRCUMSTANCES'\n",
    "        ends ='RELEVANT'\n",
    "        D = extract_text(starts, ends, cases, violation)\n",
    "    if part == 'procedure':\n",
    "        starts = 'PROCEDURE'\n",
    "        ends ='THE FACTS'\n",
    "        D = extract_text(starts, ends, cases, violation)\n",
    "    if part == 'procedure+facts':\n",
    "        starts = 'PROCEDURE'\n",
    "        ends ='THE LAW'\n",
    "        D = extract_text(starts, ends, cases, violation)\n",
    "    return D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_cross_val(Xtrain, Ytrain, vec, c, cv):\n",
    "    pipeline = Pipeline([('features', FeatureUnion([vec], )),\n",
    "                         ('classifier', LinearSVC(C=c))])\n",
    "    acc = cross_validate(\n",
    "        pipeline, Xtrain, Ytrain, cv=cv)  #10-fold cross-validation\n",
    "    return acc['test_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline(part, vec, c,article, n_shuffles=10, cv=10):  #run tests\n",
    "\n",
    "    v = extract_parts(path + 'train/' + article + '/violation/*.txt',\n",
    "                      'violation', part)\n",
    "    nv = extract_parts(path + 'train/' + article + '/non-violation/*.txt',\n",
    "                       'non-violation', part)\n",
    "    v_t = extract_parts(path + 'test20/' + article + '/violation/*.txt',\n",
    "                      'violation', part)\n",
    "    nv_t = extract_parts(path + 'test20/' + article + '/non-violation/*.txt',\n",
    "                       'non-violation', part)\n",
    "    original_dataset = v + nv + v_t + nv_t\n",
    "    accuracy= np.zeros((n_shuffles, cv), dtype=float)\n",
    "    for i in range(n_shuffles):\n",
    "        print('*** '+str(cv)+'-fold cross-validation - shuffle '+str(i)+' ***')\n",
    "        dataset = original_dataset.copy()\n",
    "        random.seed(random_seed[i])\n",
    "        shuffle(dataset)\n",
    "        Xtrain = [i[0] for i in dataset]\n",
    "        Ytrain = [i[1] for i in dataset]\n",
    "        test_score=train_model_cross_val(Xtrain, Ytrain, vec, c, cv)\n",
    "        for j in range(cv) :\n",
    "            accuracy[i][j]=test_score[j]\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article05\n",
      "*** 10-fold cross-validation - shuffle 0 ***\n",
      "*** 10-fold cross-validation - shuffle 1 ***\n",
      "*** 10-fold cross-validation - shuffle 2 ***\n",
      "*** 10-fold cross-validation - shuffle 3 ***\n",
      "*** 10-fold cross-validation - shuffle 4 ***\n",
      "*** 10-fold cross-validation - shuffle 5 ***\n",
      "*** 10-fold cross-validation - shuffle 6 ***\n",
      "*** 10-fold cross-validation - shuffle 7 ***\n",
      "*** 10-fold cross-validation - shuffle 8 ***\n",
      "*** 10-fold cross-validation - shuffle 9 ***\n",
      "Article05\n",
      "Mean:  0.7527192982456142 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    ##INDICATE THE PATH TO THE DATA\n",
    "    path = 'crystal_ball_data/'\n",
    "    #articles = ['Article02', 'Article03', 'Article05', 'Article06', 'Article08', 'Article10', 'Article11', 'Article13', 'Article14']\n",
    "    articles = [\n",
    "        'Article02', 'Article03', 'Article05', 'Article06', 'Article08',\n",
    "        'Article10', 'Article11', 'Article13', 'Article14'\n",
    "    ]\n",
    "    total_accuracy = []\n",
    "    for article in articles:  #the parameters were determined using grid-search\n",
    "        print(article)\n",
    "        if article == 'Article02':\n",
    "            vec = ('wordvec',\n",
    "                   TfidfVectorizer(\n",
    "                       analyzer='word',\n",
    "                       ngram_range=(3, 4),\n",
    "                       binary=False,\n",
    "                       lowercase=True,\n",
    "                       min_df=2,\n",
    "                       norm='l2',\n",
    "                       stop_words='english',\n",
    "                       use_idf=True))\n",
    "            c = 0.1\n",
    "            total_accuracy.append(\n",
    "                [article,\n",
    "                 run_pipeline('procedure+facts', vec, c, article)])\n",
    "        if article == 'Article03':\n",
    "            vec = ('wordvec',\n",
    "                   TfidfVectorizer(\n",
    "                       analyzer='word',\n",
    "                       binary=True,\n",
    "                       lowercase=True,\n",
    "                       min_df=2,\n",
    "                       ngram_range=(1, 1),\n",
    "                       norm='l2',\n",
    "                       stop_words='english',\n",
    "                       use_idf=True))\n",
    "            c = 0.1\n",
    "            total_accuracy.append(\n",
    "                [article,\n",
    "                 run_pipeline('procedure+facts', vec, c, article)])\n",
    "        if article == 'Article05':\n",
    "            vec = ('wordvec',\n",
    "                   TfidfVectorizer(\n",
    "                       analyzer='word',\n",
    "                       binary=False,\n",
    "                       lowercase=True,\n",
    "                       min_df=3,\n",
    "                       ngram_range=(1, 1),\n",
    "                       norm='l2',\n",
    "                       stop_words='english',\n",
    "                       use_idf=True))\n",
    "            c = 1\n",
    "            total_accuracy.append(\n",
    "                [article,\n",
    "                 run_pipeline('procedure+facts', vec, c, article)])\n",
    "        if article == 'Article06':\n",
    "            vec = ('wordvec',\n",
    "                   TfidfVectorizer(\n",
    "                       analyzer='word',\n",
    "                       binary=False,\n",
    "                       lowercase=True,\n",
    "                       min_df=2,\n",
    "                       ngram_range=(2, 4),\n",
    "                       norm='l2',\n",
    "                       stop_words='english',\n",
    "                       use_idf=True))\n",
    "            c = 5\n",
    "            total_accuracy.append(\n",
    "                [article, run_pipeline('procedure+facts', vec, c)])\n",
    "        if article == 'Article08':\n",
    "            vec = ('wordvec',\n",
    "                   TfidfVectorizer(\n",
    "                       analyzer='word',\n",
    "                       binary=True,\n",
    "                       lowercase=True,\n",
    "                       min_df=1,\n",
    "                       ngram_range=(3, 3),\n",
    "                       norm='l2',\n",
    "                       stop_words='english',\n",
    "                       use_idf=False))\n",
    "            c = 1\n",
    "            total_accuracy.append(\n",
    "                [article,\n",
    "                 run_pipeline('procedure+facts', vec, c, article)])\n",
    "        if article == 'Article10':\n",
    "            vec = ('wordvec',\n",
    "                   TfidfVectorizer(\n",
    "                       analyzer='word',\n",
    "                       binary=False,\n",
    "                       lowercase=True,\n",
    "                       min_df=1,\n",
    "                       ngram_range=(1, 1),\n",
    "                       norm='l2',\n",
    "                       stop_words='english',\n",
    "                       use_idf=False))\n",
    "            c = 5\n",
    "            total_accuracy.append(\n",
    "                [article, run_pipeline('procedure+facts', vec, c)])\n",
    "        if article == 'Article11':\n",
    "            vec = ('wordvec',\n",
    "                   TfidfVectorizer(\n",
    "                       analyzer='word',\n",
    "                       binary=False,\n",
    "                       lowercase=True,\n",
    "                       min_df=2,\n",
    "                       ngram_range=(1, 1),\n",
    "                       norm='l1',\n",
    "                       stop_words='english',\n",
    "                       use_idf=False))\n",
    "            c = 1\n",
    "            total_accuracy.append(\n",
    "                [article, run_pipeline('procedure+facts', vec, c)])\n",
    "        if article == 'Article13':\n",
    "            vec = ('wordvec',\n",
    "                   TfidfVectorizer(\n",
    "                       analyzer='word',\n",
    "                       binary=False,\n",
    "                       lowercase=True,\n",
    "                       min_df=1,\n",
    "                       ngram_range=(1, 2),\n",
    "                       norm='l2',\n",
    "                       stop_words='english',\n",
    "                       use_idf=True))\n",
    "            c = 5\n",
    "            total_accuracy.append(\n",
    "                [article, run_pipeline('procedure+facts', vec, c)])\n",
    "        if article == 'Article14':\n",
    "            vec = ('wordvec',\n",
    "                   TfidfVectorizer(\n",
    "                       analyzer='word',\n",
    "                       binary=True,\n",
    "                       lowercase=True,\n",
    "                       min_df=3,\n",
    "                       ngram_range=(1, 1),\n",
    "                       norm='l2',\n",
    "                       stop_words='english',\n",
    "                       use_idf=True))\n",
    "            c = 5\n",
    "            total_accuracy.append(\n",
    "                [article, run_pipeline('procedure+facts', vec, c)])\n",
    "\n",
    "    for element in total_accuracy:\n",
    "        print(element[0])\n",
    "        print(\"Mean: \", element[1].mean(), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for res in total_accuracy :\n",
    "    np.savetxt(\"medvedeva/\"+res[0],res[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:  0.8005476190476188\n",
      "min:  0.6944444444444444\n",
      "max:  0.9142857142857143\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.80123016, 0.80166667, 0.8027381 , 0.79710317, 0.80293651,\n",
       "       0.80571429, 0.8025    , 0.79579365, 0.80277778, 0.79301587])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.matshow(total_accuracy[0][1], vmin=0.6, vmax=0.75)\n",
    "\n",
    "print(\"mean: \", total_accuracy[0][1].mean())\n",
    "print(\"min: \", total_accuracy[0][1].min())\n",
    "print(\"max: \", total_accuracy[0][1].max())\n",
    "total_accuracy[0][1].mean(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
