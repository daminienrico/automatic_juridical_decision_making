{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL 3 - Grid search\n",
    "\n",
    "This notebook shows the gridseacrh on the Doc2Vec parameters of the model 3. See the documenation for further details.\n",
    "\n",
    "#### The preprocessing we apply is the simple one; \n",
    "#### We consider the dataset union of violations and train samples;\n",
    "#### The Doc2vec model is trained on all the possible samples;\n",
    "#### The SVM is trained on the train of the N-th article and tested on the test20 of the N-th article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import random\n",
    "import re\n",
    "import regex\n",
    "import string\n",
    "from random import shuffle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import KFold\n",
    "from gensim import models\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from sklearn import svm, metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.preprocessing import normalize\n",
    "from module_preprocessing import apply_preprocessing\n",
    "from ast import literal_eval\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from time import time\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset violations loaded. Shape:  (8388, 2)\n"
     ]
    }
   ],
   "source": [
    "data_violations = pd.read_csv(\n",
    "    \"crystal_ball_data/SIMPLE_PREP/all_violations_simple_rd.csv\",\n",
    "    index_col=\"index\")\n",
    "data_violations.raw_text = data_violations.raw_text.apply(literal_eval)\n",
    "print(\"dataset violations loaded. Shape: \", data_violations.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset train loaded. Shape:  (3125, 2)\n",
      "dataset violations + train created. Shape:  (11513, 2)\n",
      "corpus tagged. Length:  11513\n"
     ]
    }
   ],
   "source": [
    "data_train = pd.read_csv(\n",
    "    \"crystal_ball_data/SIMPLE_PREP/all_train_simple_rd.csv\", index_col=\"index\")\n",
    "data_train.raw_text = data_train.raw_text.apply(literal_eval)\n",
    "print(\"dataset train loaded. Shape: \", data_train.shape)\n",
    "\n",
    "data_vt = data_violations.append(data_train)\n",
    "print(\"dataset violations + train created. Shape: \", data_vt.shape)\n",
    "\n",
    "tagged_documents = []\n",
    "for index, row in data_vt.iterrows():\n",
    "    tagged_documents.append(\n",
    "        TaggedDocument(words=row['raw_text'], tags=[(row['tag'])]))\n",
    "print(\"corpus tagged. Length: \", len(tagged_documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset Article02 has been loaded. Shape:  (26, 2)\n",
      "dataset Article03 has been loaded. Shape:  (140, 2)\n",
      "dataset Article05 has been loaded. Shape:  (74, 2)\n",
      "dataset Article06 has been loaded. Shape:  (226, 2)\n",
      "dataset Article08 has been loaded. Shape:  (111, 2)\n",
      "dataset Article10 has been loaded. Shape:  (52, 2)\n",
      "dataset Article11 has been loaded. Shape:  (14, 2)\n",
      "dataset Article13 has been loaded. Shape:  (52, 2)\n",
      "dataset Article14 has been loaded. Shape:  (70, 2)\n"
     ]
    }
   ],
   "source": [
    "path = \"crystal_ball_data/SIMPLE_PREP/test_RAW_DATASET/\"\n",
    "datasets = []\n",
    "for filename in os.listdir(path)[1:]:\n",
    "    dataset = pd.read_csv(path + filename, index_col=\"index\")\n",
    "    dataset.raw_text = dataset.raw_text.apply(literal_eval)\n",
    "    random.seed(6789)\n",
    "    dataset = dataset.sample(frac=1, random_state=6789).reset_index(drop=True)\n",
    "    print(\"dataset \" + filename[:9] + \" has been loaded. Shape: \",dataset.shape)\n",
    "    datasets.append([dataset, filename[:9]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset Article02 has been loaded. Shape:  (112, 2)\n",
      "dataset Article03 has been loaded. Shape:  (565, 2)\n",
      "dataset Article05 has been loaded. Shape:  (298, 2)\n",
      "dataset Article06 has been loaded. Shape:  (914, 2)\n",
      "dataset Article08 has been loaded. Shape:  (456, 2)\n",
      "dataset Article10 has been loaded. Shape:  (210, 2)\n",
      "dataset Article11 has been loaded. Shape:  (62, 2)\n",
      "dataset Article13 has been loaded. Shape:  (208, 2)\n",
      "dataset Article14 has been loaded. Shape:  (286, 2)\n"
     ]
    }
   ],
   "source": [
    "path = \"crystal_ball_data/SIMPLE_PREP/train_RAW_DATASET/\"\n",
    "datasets_train = []\n",
    "for filename in os.listdir(path)[1:]:\n",
    "    dataset = pd.read_csv(path + filename, index_col=\"index\")\n",
    "    dataset.raw_text = dataset.raw_text.apply(literal_eval)\n",
    "    random.seed(6789)\n",
    "    dataset = dataset.sample(frac=1, random_state=6789).reset_index(drop=True)\n",
    "    datasets_train.append([dataset, filename[:9]])\n",
    "    print(\"dataset \" + filename[:9] + \" has been loaded. Shape: \",dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vector_dataset(df, model, verbose):\n",
    "    '''\n",
    "     This function creates a well structured dataset, which consists of n features and the tag.\n",
    "     It transforms strings to vectors through the model Doc2Vec.\n",
    "     which consists of n columns: n features and the tag. \n",
    "      Args:\n",
    "         df (DataFrame): an semi-structured dataframe, look at create_raw_dataset.\n",
    "         df_vector (DataFrame): an initialized dataframe with shape (n_features+1,).\n",
    "         model (Doc2Vec) : a trained Doc2Vec model.\n",
    "      Returns :\n",
    "         (DataFrame) a Dataframe with shape (n_features+1,n_samples). \n",
    "    '''\n",
    "    df_vector = pd.DataFrame()\n",
    "    for index, row in df.iterrows():\n",
    "        vector = model.infer_vector(row['raw_text'])\n",
    "        X = {}\n",
    "        for i in range(len(vector)):\n",
    "            X['y_' + str(i)] = vector[i]\n",
    "        X['tag'] = row['tag']\n",
    "        df_vector = df_vector.append(X, ignore_index=True)\n",
    "    df_vector.loc[:, 'y_0':'y_' + str(len(vector) - 1)] = normalize(\n",
    "        df_vector.loc[:, 'y_0':'y_' + str(len(vector) - 1)], norm='l2', axis=0)\n",
    "    if verbose >= 2:\n",
    "        print(df_vector.head(5))\n",
    "    return df_vector, len(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_once(df_test, df_train, model_D2V, verbose=0):\n",
    "    #creating the dataset through Doc2Vec for feeding the SVM\n",
    "    df_vector, vector_size = create_vector_dataset(df_train, model_D2V,\n",
    "                                                   verbose)\n",
    "    df_test_vector, _ = create_vector_dataset(df_test, model_D2V, verbose)\n",
    "    print(\"Training the SVM with\", len(df_vector), \"samples\")\n",
    "    print(\"Testing the SVM with\", len(df_test_vector), \"samples\")\n",
    "    #fitting on train\n",
    "    clf_svm = svm.LinearSVC(C=1)\n",
    "    clf_svm.fit(df_vector.loc[:, 'y_0':'y_' + str(vector_size - 1)].values,\n",
    "                df_vector.loc[:, 'tag'].values)\n",
    "    #testing on test20\n",
    "    predictions = clf_svm.predict(\n",
    "        df_test_vector.loc[:, 'y_0':'y_' + str(vector_size - 1)].values)\n",
    "    tag_test = df_test_vector.loc[:, 'tag'].values\n",
    "    acc = metrics.accuracy_score(tag_test, predictions)\n",
    "    if verbose >= 0:\n",
    "        print(\"ACCURACY:\", acc)\n",
    "        #print(\"Precision:\",metrics.precision_score(tag_test, predictions))\n",
    "        #print(\"Recall:\",metrics.recall_score(tag_test, predictions))\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    " def main() :\n",
    "    summary = {}\n",
    "    param_grid = {\n",
    "        'vector_size': [100],\n",
    "        'epochs': [20],\n",
    "        'min_count': [100],\n",
    "        'window': [10],\n",
    "        'hs': [1],\n",
    "        'negative': [20],\n",
    "        'ns_exponent': [0.75],\n",
    "        'dm_mean': [0],\n",
    "        'dbow_words': [0]\n",
    "    }    \n",
    "    parameters = list(ParameterGrid(param_grid))\n",
    "    print(\"\\nTotal combination of parameters: %d\" % len(parameters))\n",
    "    for parameter in parameters:\n",
    "        print(\"parameters:\", parameter)\n",
    "        start = time()\n",
    "        model_D2V = Doc2Vec(\n",
    "            tagged_documents,\n",
    "            negative=parameter['negative'],\n",
    "            ns_exponent=parameter['ns_exponent'],\n",
    "            hs=parameter['hs'],\n",
    "            window=parameter['window'],\n",
    "            dm_mean=parameter['dm_mean'],\n",
    "            dbow_words=parameter['dbow_words'],\n",
    "            vector_size=parameter['vector_size'],\n",
    "            epochs=parameter['epochs'],\n",
    "            min_count=parameter['min_count'],\n",
    "            workers=os.cpu_count())\n",
    "        print(\"model Doc2Vec created. Time elasped: \"+str(timedelta(seconds=(time() - start))))\n",
    "        l = []\n",
    "        for dataset, filename in datasets:\n",
    "            print(\"\\n\", filename)\n",
    "            print(\"Number of samples: \", dataset.shape[0])\n",
    "            train = pd.DataFrame()\n",
    "            for d in datasets_train:\n",
    "                if d[1] == filename:\n",
    "                    train = d[0]\n",
    "                    print(\"fitting \", d[1])\n",
    "                    break\n",
    "            acc = fit_once(dataset, train, model_D2V, verbose=0)\n",
    "            l.append((filename, acc))\n",
    "        summary.update({str(parameter): l})\n",
    "\n",
    "    print(\"Finished.\")\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total combination of parameters: 1\n",
      "parameters: {'dbow_words': 0, 'dm_mean': 0, 'epochs': 20, 'hs': 1, 'min_count': 100, 'negative': 20, 'ns_exponent': 0.75, 'vector_size': 100, 'window': 10}\n",
      "model Doc2Vec created. Time elasped: 0:20:19.763975\n",
      "\n",
      " Article02\n",
      "Number of samples:  26\n",
      "fitting  Article02\n",
      "Training the SVM with 112 samples\n",
      "Testing the SVM with 26 samples\n",
      "ACCURACY: 0.7692307692307693\n",
      "\n",
      " Article03\n",
      "Number of samples:  140\n",
      "fitting  Article03\n",
      "Training the SVM with 565 samples\n",
      "Testing the SVM with 140 samples\n",
      "ACCURACY: 0.7642857142857142\n",
      "\n",
      " Article05\n",
      "Number of samples:  74\n",
      "fitting  Article05\n",
      "Training the SVM with 298 samples\n",
      "Testing the SVM with 74 samples\n",
      "ACCURACY: 0.7027027027027027\n",
      "\n",
      " Article06\n",
      "Number of samples:  226\n",
      "fitting  Article06\n",
      "Training the SVM with 914 samples\n",
      "Testing the SVM with 226 samples\n",
      "ACCURACY: 0.7433628318584071\n",
      "\n",
      " Article08\n",
      "Number of samples:  111\n",
      "fitting  Article08\n",
      "Training the SVM with 456 samples\n",
      "Testing the SVM with 111 samples\n",
      "ACCURACY: 0.7027027027027027\n",
      "\n",
      " Article10\n",
      "Number of samples:  52\n",
      "fitting  Article10\n",
      "Training the SVM with 210 samples\n",
      "Testing the SVM with 52 samples\n",
      "ACCURACY: 0.5576923076923077\n",
      "\n",
      " Article11\n",
      "Number of samples:  14\n",
      "fitting  Article11\n",
      "Training the SVM with 62 samples\n",
      "Testing the SVM with 14 samples\n",
      "ACCURACY: 0.6428571428571429\n",
      "\n",
      " Article13\n",
      "Number of samples:  52\n",
      "fitting  Article13\n",
      "Training the SVM with 208 samples\n",
      "Testing the SVM with 52 samples\n",
      "ACCURACY: 0.7307692307692307\n",
      "\n",
      " Article14\n",
      "Number of samples:  70\n",
      "fitting  Article14\n",
      "Training the SVM with 286 samples\n",
      "Testing the SVM with 70 samples\n",
      "ACCURACY: 0.7428571428571429\n",
      "Finished.\n",
      "CPU times: user 59min 49s, sys: 35.8 s, total: 1h 25s\n",
      "Wall time: 23min 44s\n"
     ]
    }
   ],
   "source": [
    "%time summary=main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parameters:  {'dbow_words': 0, 'dm_mean': 0, 'epochs': 20, 'hs': 1, 'min_count': 100, 'negative': 20, 'ns_exponent': 0.75, 'vector_size': 100, 'window': 10}\n",
      "Total average: 0.706273\n",
      "BEST PARAMETER:  {'dbow_words': 0, 'dm_mean': 0, 'epochs': 20, 'hs': 1, 'min_count': 100, 'negative': 20, 'ns_exponent': 0.75, 'vector_size': 100, 'window': 10} \n",
      "BEST ACC: 0.7062733938840133\n"
     ]
    }
   ],
   "source": [
    "max_a = 0\n",
    "for i in summary:\n",
    "    print(\"\\nParameters: \", i)\n",
    "    total_average = 0\n",
    "    for j in range(len(summary[i])):\n",
    "        #print(summary[i][j][0],\"average: %f\" % summary[i][j][1])\n",
    "        total_average += summary[i][j][1]\n",
    "    total_average = total_average / len(summary[i])\n",
    "    if total_average > max_a:\n",
    "        max_a = total_average\n",
    "        best_par = i\n",
    "    print(\"Total average: %f\" % total_average)\n",
    "print(\"BEST PARAMETER: \", best_par, \"\\nBEST ACC:\", max_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parameters:  {'dbow_words': 1, 'dm_mean': 1, 'epochs': 20, 'hs': 0, 'min_count': 1, 'negative': 20, 'ns_exponent': 0.75, 'vector_size': 500, 'window': 3}\n",
      "Total average: 0.733849\n",
      "BEST PARAMETER:  {'dbow_words': 1, 'dm_mean': 1, 'epochs': 20, 'hs': 0, 'min_count': 1, 'negative': 20, 'ns_exponent': 0.75, 'vector_size': 500, 'window': 3} \n",
      "BEST ACC: 0.7338490294242506\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parameters:  {'dbow_words': 0, 'dm_mean': 0, 'epochs': 20, 'hs': 0, 'min_count': 1, 'negative': 5, 'ns_exponent': 1, 'vector_size': 500, 'window': 3}\n",
      "Total average: 0.737149\n",
      "BEST PARAMETER:  {'dbow_words': 0, 'dm_mean': 0, 'epochs': 20, 'hs': 0, 'min_count': 1, 'negative': 5, 'ns_exponent': 1, 'vector_size': 500, 'window': 3} \n",
      "BEST ACC: 0.7371488331665322\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "386px",
    "left": "819px",
    "right": "20px",
    "top": "82px",
    "width": "505px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
